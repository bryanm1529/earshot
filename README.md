# ğŸ§  Earshot Cognitive Co-Pilot

> **Python-Native Real-time AI Assistant for Live Audio Streams**

A streamlined cognitive co-pilot system that provides intelligent, context-aware assistance during conversations, meetings, and presentations. Built with a robust Python-native architecture that eliminates complex dependencies and Just Worksâ„¢ on Windows.

[![Windows Native](https://img.shields.io/badge/Windows-Native-blue?logo=windows&logoColor=white)](#)
[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)](#)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE.md)

## âœ¨ Features

- **ğŸ¤ Real-time Audio Processing**: Live transcription using Whisper CLI with subprocess management
- **ğŸ§  Intelligent Question Detection**: AI-powered regex patterns for instant question recognition
- **ğŸ’¡ Contextual Assistance**: LLM-powered responses using Ollama with conversation memory
- **ğŸŒ WebSocket Integration**: Real-time communication with frontend HUD interface
- **ğŸ–¥ï¸ Windows Native**: Robust, simplified architecture with zero C++ networking issues
- **ğŸ”’ Privacy-First**: All processing happens locally - no cloud dependencies
- **âš¡ Low Latency**: Sub-second response times with optimized Python pipeline
- **ğŸ“Š Production Ready**: Robust error handling, monitoring, and graceful shutdown

## ğŸš€ Quick Start

### Two-Part Launch Process

**Part 1: Start Backend (Headless Services)**
```powershell
# Terminal 1: Start the Python-native backend
.\start_simple.ps1
```

**Part 2: Start Frontend (Visual HUD)**
```powershell
# Terminal 2: Start the Tauri HUD (when backend is ready)
.\start_frontend.ps1
```

### One-Time Setup
```powershell
# 1. Clone and setup
git clone https://github.com/your-username/earshot-copilot.git
cd earshot-copilot
.\setup_windows.ps1

# 2. Install frontend dependencies
cd frontend
pnpm install
```

## ğŸ“‹ Prerequisites

- **Windows 10/11** (64-bit)
- **Python 3.10+** (with PATH configuration)
- **Node.js LTS + pnpm** (for frontend)
- **Rust + Tauri CLI** (for HUD)
- **FFmpeg** (for audio capture)
- **Ollama** (for local LLM)
- **VB-Audio Virtual Cable** (for audio routing)

## ğŸ—ï¸ Python-Native Architecture

```
Backend (Headless)          Frontend (Visual)
â”œâ”€â”€ ğŸ¤ Audio (ffmpeg)       â”œâ”€â”€ ğŸªŸ Tauri HUD Window
â”œâ”€â”€ ğŸ—£ï¸ Whisper CLI          â”œâ”€â”€ âš¡ React UI
â”œâ”€â”€ ğŸ¤– Chronicler           â”œâ”€â”€ ğŸ”Œ WebSocket Client
â”œâ”€â”€ ğŸ’¡ Advisor (Ollama)     â””â”€â”€ ğŸ¯ Overlay Display
â”œâ”€â”€ ğŸŒ WebSocket Server
â””â”€â”€ ğŸ“Š Process Management
```

### Strategic Pivot Benefits

- **ğŸš« ZERO C++ networking issues** - All handled in Python
- **ğŸ”§ Robust process management** - Python asyncio handles everything
- **ğŸ› Simpler debugging** - One backend log stream
- **ğŸ—ï¸ Modular design** - Backend/frontend completely separate
- **ğŸªŸ Windows native** - No cross-platform compatibility hell

## ğŸ”§ Configuration

The system uses sensible defaults. Optional customization via environment variables:

```env
COPILOT_ADVISOR_MODEL=llama3:8b
COPILOT_CHRONICLER_ENABLED=true
OLLAMA_HOST=127.0.0.1
OLLAMA_PORT=11434
```

## ğŸ¯ Usage

### Access Points
- **Frontend HUD**: Launched via `.\start_frontend.ps1`
- **WebSocket Server**: `ws://localhost:9082`
- **Backend Status**: Visible in Terminal 1 logs

### Daily Workflow
```powershell
# Terminal 1: Backend
.\start_simple.ps1
# Wait for: "frontend clients: 1"

# Terminal 2: Frontend
.\start_frontend.ps1
# HUD window appears and connects
```

### Audio Setup
1. Install **VB-Audio Virtual Cable** from [vb-audio.com](https://vb-audio.com/Cable/)
2. Route desired audio to "CABLE Input"
3. System automatically captures from "CABLE Output"

## ğŸ”§ Development & Testing

### Backend Development
```powershell
# Test all components
cd backend
python test_native_setup.py

# Run backend only
python brain_native.py --debug
```

### Frontend Development
```powershell
cd frontend
pnpm tauri dev
```

## ğŸ› Troubleshooting

### System Validation
```powershell
# Run comprehensive tests
cd backend
python test_native_setup.py
```

### Common Issues
- **Backend not starting**: Check Ollama is running (`ollama serve`)
- **No audio**: Verify VB-Audio Virtual Cable installation
- **Frontend won't connect**: Ensure backend started first
- **Model errors**: Pull required model (`ollama pull llama3:8b`)

## ğŸ“ Project Structure

```
earshot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ brain_native.py          # ğŸ§  Main Python-native engine
â”‚   â”œâ”€â”€ test_native_setup.py     # ğŸ§ª System validation
â”‚   â”œâ”€â”€ whisper.cpp/             # ğŸ—£ï¸ CLI tools & models
â”‚   â””â”€â”€ requirements.txt         # ğŸ“¦ Dependencies
â”œâ”€â”€ frontend/                    # ğŸ¨ Tauri/React HUD
â”œâ”€â”€ start_simple.ps1            # ğŸš€ Backend launcher
â”œâ”€â”€ start_frontend.ps1          # ğŸ–¥ï¸ Frontend launcher
â””â”€â”€ NATIVE_ARCHITECTURE_GUIDE.md # ğŸ“– Detailed guide
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Test your changes: `python backend/test_native_setup.py`
4. Commit changes: `git commit -m 'Add amazing feature'`
5. Push to branch: `git push origin feature/amazing-feature`
6. Open a Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

## ğŸ™ Acknowledgments

- [Whisper.cpp](https://github.com/ggerganov/whisper.cpp) for local speech recognition
- [Ollama](https://ollama.ai/) for local LLM inference
- [Tauri](https://tauri.app/) for cross-platform desktop applications

---

<div align="center">

**Ready to enhance your conversations with AI assistance?**

[ğŸ“– Architecture Guide](NATIVE_ARCHITECTURE_GUIDE.md) â€¢ [ğŸš€ Quick Start](#quick-start) â€¢ [ğŸ§ª System Test](backend/test_native_setup.py)

*Built with â¤ï¸ for a robust, Python-native experience*

</div>
